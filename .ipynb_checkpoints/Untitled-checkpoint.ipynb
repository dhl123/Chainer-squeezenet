{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from squeezenet import *\n",
    "from chainer import serializers\n",
    "from chainer import optimizers\n",
    "from chainer import training,iterators\n",
    "from chainer.training import extensions\n",
    "from chainer.datasets import TupleDataset\n",
    "import numpy as np\n",
    "import glob\n",
    "import os, sys, shutil\n",
    "from leNet import *\n",
    "\n",
    "model=SqueezeNet()\n",
    "\n",
    "batchsize=32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split datasets to 'train'/'test'\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import os,shutil\n",
    "def get_name_list(filepath):                #获取各个类别的名字  \n",
    "    pathDir =  os.listdir(filepath)  \n",
    "    out = []  \n",
    "    for allDir in pathDir:  \n",
    "        if os.path.isdir(os.path.join(filepath,allDir)):  \n",
    "            #child = allDir.decode('gbk')    # .decode('gbk')是解决中文显示乱码问题  \n",
    "            out.append(allDir)  \n",
    "    return out \n",
    "#print(os.listdir(\"imgnet/train/n01498041.tar\"))\n",
    "def creatdata(num=0,filepath=\"imgnet/train\"):\n",
    "    catag=get_name_list(filepath)\n",
    "    filenames=[]\n",
    "    for i in range(len(catag)):\n",
    "        print(os.path.join(filepath,catag[i]))\n",
    "        filenames.append(os.listdir(os.path.join(filepath,catag[i])))\n",
    "        print(len(filenames[i]))\n",
    "        for j in range(num):\n",
    "            fpath=os.path.join(\"imgnet/test\",catag[i],filenames[i][j])\n",
    "            if not os.path.exists(os.path.dirname(fpath)):\n",
    "                print(fpath)\n",
    "                os.makedirs(os.path.dirname(fpath))  \n",
    "            print(os.path.join(filepath,catag[i],filenames[i][j]))\n",
    "             \n",
    "            shutil.move(\n",
    "                os.path.join(filepath,catag[i],filenames[i][j]),\n",
    "                os.path.join(\"./imgnet/test\",catag[i],filenames[i][j])\n",
    "            )\n",
    "    \n",
    "creatdata(330)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LabeledImageDataset\n",
    "\n",
    "\n",
    "try:\n",
    "    from PIL import Image\n",
    "    available = True\n",
    "except ImportError as e:\n",
    "    available = False\n",
    "    _import_error = e\n",
    "import six  \n",
    "\n",
    "from chainer.dataset import dataset_mixin\n",
    "\n",
    "def _read_image_as_array(path, dtype):\n",
    "    f = Image.open(path)\n",
    "    f=f.resize((227,227))\n",
    "    try:\n",
    "        image = np.asarray(f, dtype=dtype)\n",
    "    finally:\n",
    "        # Only pillow >= 3.0 has 'close' method\n",
    "        if hasattr(f, 'close'):\n",
    "            f.close()\n",
    "    return image\n",
    "\n",
    "class LabeledImageDataset(dataset_mixin.DatasetMixin):\n",
    "\n",
    "\n",
    "    def __init__(self, pairs, root='.', dtype=np.float32,\n",
    "                 label_dtype=np.int32):\n",
    "        _check_pillow_availability()\n",
    "        # print(pairs)\n",
    "        if isinstance(pairs, six.string_types):\n",
    "            pairs_path = pairs\n",
    "            with open(pairs_path) as pairs_file:\n",
    "                pairs = []\n",
    "                for i, line in enumerate(pairs_file):\n",
    "                    pair = line.strip().split()\n",
    "                    if len(pair) != 2:\n",
    "                        raise ValueError(\n",
    "                            'invalid format at line {} in file {}'.format(\n",
    "                                i, pairs_path))\n",
    "                    pairs.append((pair[0], int(pair[1])))\n",
    "        \n",
    "        self._pairs = pairs\n",
    "        self._root = root\n",
    "        self._dtype = dtype\n",
    "        self._label_dtype = label_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._pairs)\n",
    "\n",
    "    def get_example(self, i):\n",
    "        \n",
    "        path, int_label = self._pairs[i]\n",
    "        #print(path)\n",
    "        # print(int_label)\n",
    "        full_path = os.path.join(self._root, path)\n",
    "        image = _read_image_as_array(full_path, self._dtype)\n",
    "\n",
    "        if image.ndim == 2:\n",
    "            # image is greyscale\n",
    "            \n",
    "            image = image[:, :, np.newaxis]\n",
    "        label = np.array(int_label, dtype=self._label_dtype)\n",
    "        return image.transpose(2, 0, 1), label\n",
    "    \n",
    "def _check_pillow_availability():\n",
    "    if not available:\n",
    "        raise ImportError('PIL cannot be loaded. Install Pillow!\\n'\n",
    "                          'The actual import error is as follows:\\n' +\n",
    "                          str(_import_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def get_name_list(filepath):                #获取各个类别的名字  \n",
    "    pathDir =  os.listdir(filepath)  \n",
    "    out = []  \n",
    "    for allDir in pathDir:  \n",
    "        if os.path.isdir(os.path.join(filepath,allDir)):  \n",
    "            out.append(allDir)  \n",
    "    return out \n",
    "\n",
    "\n",
    "#get all image files names\n",
    "def getallnames():\n",
    "    train_name=get_name_list(\"images\")\n",
    "    # test_name=get_name_list(\"imgnet/test\")\n",
    "    names=[]\n",
    "    for i in range(len(train_name)):\n",
    "        name=os.listdir(os.path.join(\"images\",train_name[i]))\n",
    "        for j in name:\n",
    "            names.append(os.path.join(\"images\",train_name[i],j))\n",
    "    # for i in range(len(test_name)):\n",
    "    #     name=os.listdir(os.path.join(\"imgnet/test\",test_name[i]))\n",
    "    #    for j in name:\n",
    "    #        names.append(os.path.join(\"imgnet/test\",test_name[i],j))\n",
    "\n",
    "    return names\n",
    "#get L images names\n",
    "def getLnames(names):\n",
    "    lists=[]\n",
    "    for path in names:\n",
    "        #print(path)\n",
    "        im = Image.open(path)\n",
    "        if(im.mode!=\"RGB\"):\n",
    "            lists.append(path)\n",
    "    return lists\n",
    "\n",
    "\n",
    "#return LabeledDatasets  \n",
    "def create_dataset(root_dir):\n",
    "    data_pairs = []\n",
    "    Lnames=getLnames(getallnames())\n",
    "    la=0\n",
    "    for label_dir in os.listdir(root_dir):\n",
    "        print(label_dir)\n",
    "        label = la\n",
    "        la=la+1\n",
    "        pa=(os.path.join(root_dir, label_dir))\n",
    "        files=[]\n",
    "        for f in os.listdir(pa):\n",
    "            if os.path.join(pa,f) not in Lnames:\n",
    "                files.append(f)\n",
    "               # print(os.path.join(pa,f))\n",
    "        \n",
    "        data_pairs += [(os.path.join(label_dir, f), label) for f in files]\n",
    "        \n",
    "    return LabeledImageDataset(pairs=data_pairs, root=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save(filename,data):\n",
    "    output=open(filename,'wb')\n",
    "    pickle.dump(data,output)\n",
    "    output.close()\n",
    "    \n",
    "def load(filename):\n",
    "    pik_file=open(filename,'rb')\n",
    "    data=pickle.load(pik_file)\n",
    "    pik_file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer import datasets\n",
    "train_name=[]\n",
    "test_name=[]\n",
    "full_dataset = create_dataset(\"images\")\n",
    "train_name, test_name = datasets.split_dataset_random(full_dataset, int(len(full_dataset) * 0.8), seed=0)\n",
    "train_iter = iterators.SerialIterator(train_name, batchsize)\n",
    "test_iter = iterators.SerialIterator(test_name, batchsize,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''get weights and save\n",
    "\n",
    "\n",
    "serializers.load_npz('weights.npz', model)\n",
    "conv1=getattr(model, 'conv1')\n",
    "fire2=getattr(model, 'fire2')\n",
    "fire3=getattr(model, 'fire3')\n",
    "fire4=getattr(model, 'fire4')\n",
    "fire5=getattr(model, 'fire5')\n",
    "fire6=getattr(model, 'fire6')\n",
    "fire7=getattr(model, 'fire7')\n",
    "fire8=getattr(model, 'fire8')\n",
    "fire9=getattr(model, 'fire9')\n",
    "conv10=getattr(model,'conv10')\n",
    "\n",
    "fires=[]\n",
    "fires.append(conv1)\n",
    "fires.append(conv10)\n",
    "for i in range(2,10):\n",
    "    fire=eval(\"fire\"+str(i))\n",
    "    conv1=getattr(fire,'conv1')\n",
    "    conv2=getattr(fire,'conv2')\n",
    "    conv3=getattr(fire,'conv3')\n",
    "    bn=getattr(fire,'bn4')\n",
    "    fir=[]\n",
    "    fir.append(conv1)\n",
    "    fir.append(conv2)\n",
    "    fir.append(conv3)\n",
    "    fir.append(bn)\n",
    "    fires.append(fir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train data\n",
    "import numpy as np\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.cuda import to_cpu, to_gpu\n",
    "from chainer import optimizers\n",
    "\n",
    "import chainer.computational_graph as c\n",
    "\n",
    "optimizer = optimizers.MomentumSGD(lr=0.05,momentum=0.9)\n",
    "\n",
    "weights=load(\"weights.pkl\")\n",
    "conv1=getattr(model, 'conv1')\n",
    "fire2=getattr(model, 'fire2')\n",
    "fire3=getattr(model, 'fire3')\n",
    "fire4=getattr(model, 'fire4')\n",
    "fire5=getattr(model, 'fire5')\n",
    "fire6=getattr(model, 'fire6')\n",
    "fire7=getattr(model, 'fire7')\n",
    "fire8=getattr(model, 'fire8')\n",
    "fire9=getattr(model, 'fire9')\n",
    "conv10=getattr(model,'conv10')\n",
    "conv1.W=weights[0].W\n",
    "conv1.b=weights[0].b\n",
    "\n",
    "for i in range(2,10):\n",
    "    name=eval(\"fire\"+str(i))\n",
    "    getattr(name,\"conv1\").W=weights[i][0].W\n",
    "    getattr(name,'conv2').W=weights[i][1].W\n",
    "    getattr(name,'conv3').W=weights[i][2].W\n",
    "    getattr(name,\"conv1\").b=weights[i][0].b\n",
    "    getattr(name,'conv2').b=weights[i][1].b\n",
    "    getattr(name,'conv3').b=weights[i][2].b\n",
    "    getattr(name,'bn4').copyparams(weights[i][3])\n",
    "    \n",
    "gpu_id=-1\n",
    "if gpu_id>=0:\n",
    "    model = model.to_gpu()\n",
    "# Give the optimizer a reference to the model so that it\n",
    "# can locate the model's parameters.\n",
    "optimizer.setup(model)\n",
    "max_epoch = 80\n",
    "\n",
    "while train_iter.epoch < max_epoch:\n",
    "\n",
    "    # ---------- One iteration of the training loop ----------\n",
    "    train_batch = train_iter.next()\n",
    "    # print(\"train_epoch\", train_batch)\n",
    "    #print([c[0].shape for c in train_batch])\n",
    "    image_train, target_train = concat_examples(train_batch, gpu_id)\n",
    "\n",
    "    # Calculate the prediction of the network\n",
    "    prediction_train = model(image_train)\n",
    "    # g = c.build_computational_graph(prediction_train)\n",
    "    # with open('./cg.dot', 'w') as o:\n",
    "    #    o.write(g.dump())\n",
    "\n",
    "    # Calculate the loss with softmax_cross_entropy\n",
    "    loss = F.softmax_cross_entropy(prediction_train, target_train)\n",
    "\n",
    "    # Calculate the gradients in the network\n",
    "    model.cleargrads()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update all the trainable paremters\n",
    "    optimizer.update()\n",
    "    # --------------------- until here ---------------------\n",
    "\n",
    "    # Check the validation accuracy of prediction after every epoch\n",
    "    if train_iter.is_new_epoch:  # If this iteration is the final iteration of the current epoch\n",
    "\n",
    "        # Display the training loss\n",
    "        print('epoch:{:02d} train_loss:{:.04f} '.format(\n",
    "            train_iter.epoch, float(to_cpu(loss.data))))\n",
    "\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "        while True:\n",
    "            test_batch = test_iter.next()\n",
    "            image_test, target_test = concat_examples(test_batch, gpu_id)\n",
    "\n",
    "            # Forward the test data\n",
    "            prediction_test = model(image_test)\n",
    "            \n",
    "\n",
    "            # Calculate the loss\n",
    "            loss_test = F.softmax_cross_entropy(prediction_test, target_test)\n",
    "            test_losses.append(to_cpu(loss_test.data))\n",
    "\n",
    "            # Calculate the accuracy\n",
    "            accuracy = F.accuracy(prediction_test, target_test)\n",
    "            accuracy.to_cpu()\n",
    "            test_accuracies.append(accuracy.data)\n",
    "\n",
    "            if test_iter.is_new_epoch:\n",
    "                test_iter.epoch = 0\n",
    "                test_iter.current_position = 0\n",
    "                test_iter.is_new_epoch = False\n",
    "                test_iter._pushed_position = None\n",
    "                break\n",
    "\n",
    "        print('val_loss:{:.04f} val_accuracy:{:.04f}'.format(\n",
    "            np.mean(test_losses), np.mean(test_accuracies)))\n",
    "        \n",
    "        ##如果结果等于1/num_class，就不用算了，就是没用的（模型根本没训练）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''%%time\n",
    "from PIL import Image\n",
    "img=Image.open(\"imgnet/data/n01514859.tar/n01514859_210.JPEG\")\n",
    "img=img.resize((227,227))\n",
    "img=np.asarray(img,dtype=np.float32)\n",
    "img=img.transpose(2, 0, 1)\n",
    "img=np.array([img])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y=model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
